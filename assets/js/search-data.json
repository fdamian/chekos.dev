{
  
    
        "post0": {
            "title": "Haciendo datos abiertos m√°s accesibles con datasette",
            "content": "California recientemente liber√≥ datos sobre las detenciones hechas por oficiales de las 8 agencias m√°s grandes del estado. Estos datos cubren los meses de julio a diciembre del 2018. Esta fue la primera ola de divulgaci√≥n de datos que entrar√° en vigencia en los a√±os siguientes. Los datos cubrieron m√°s de 1.8 millones de paradas en todo el estado. Si bien este es un paso en la direcci√≥n correcta, un solo archivo .csv de alrededor de 640 megabytes con m√°s de 1.8 millones de filas y m√°s de 140 columnas podr√≠a ser intimidante para algunas personas que se beneficiar√≠an de la exploraci√≥n de estos datos: l√≠deres locales, periodistas, activistas y organizadores, por nombrar algunos. . El tercer principio de la carta internacional de datos abiertos es que los datos deben ser accesibles y utilizables. . Proporcionar los datos es el primer paso, pero para que sean utilizables y accesibles para la mayor√≠a de las personas, no podemos simplemente publicar datos; debemos tener en cuenta la experiencia del usuario y desarrollar formas de facilitar la exploraci√≥n y el uso de dichos datos. . En este tutorial, comparto una forma de hacerlo: usando datasette, una ‚Äúherramienta de Python para explorar y publicar datos. Ayuda a las personas a tomar datos de cualquier forma o tama√±o y publicarlos como un sitio web interactivo y explorable y con una API acompa√±ante‚Äù. . Tomamos los datos de detenci√≥n policial lanzados recientemente por California, los limpiamos y transformamos a como sea necesario, y desplegamos una instancia de datasette en Heroku. . pr√≥logo . La idea de explorar este conjunto de datos vino al escuchar el episodio del 3 de marzo del 2020 del podcast Pod Save the People. (minuto 4:12) . donde mencionan un art√≠culo de The Appeal: . Boudin will announce a second directive today, also reviewed by The Appeal, on what are known as pre-textual stops, in which an officer stops someone for a minor offense or infraction, such as a traffic violation, in order to conduct an unrelated search for items like guns or drugs. According to the new policy, the DA‚Äôs office will not prosecute possession of contraband cases when the contraband was collected as a result of an infraction-related stop, ‚Äúwhere there is no other articulable suspicion of criminal activity.‚Äù Any deviations from the policy should be made in writing and require approval from the DA or a chief of the criminal division. Additionally, the ban includes cases in which a person consented to a search ‚Äúbecause of the long-standing and documented racial and ethnic disparities in law enforcement requests for consent to search,‚Äù according to the directive. - https://theappeal.org/san-francisco-da-to-announce-sweeping-changes-on-sentencing-policy-and-police-stops/ . En el episodio, Sam Sinyangwe menciona que las personas negras y de color est√°n siendo detenidas y registradas a tasas m√°s altas, y que muchos de estos registros son lo que se conocen como ‚Äòregistros consensuales‚Äô, lo que significa que la polic√≠a no informa ninguna justificaci√≥n para registrar a la persona mas que preguntar a esa persona si puede registrarlos y que la persona presuntamente da su consentimiento. . Esta gran disparidad racial es desgarradora pero no es sorpresa. . Cuando comenc√© a trabajar con el conjunto de datos me pareci√≥ un tanto √≠ncomodo, no era f√°cil. El tama√±o del conjunto de los datos hace que sea d√≠ficil trabajar con √©l para aquellas personas que no analizan datos program√°ticamente; ya sea con recursos pagados como stata y sas o gratuitos como python y R. . Esta informaci√≥n no esta dise√±ada para ser explorada f√°cilmente pero existen herramientas que pueden ayudar con eso. . üí°Nota: AB-953, el proyecto de ley que requiere que las agencias reporten todos los datos sobre paradas al Procurador General, no requiere que los datos sean f√°cilmente ‚Äúexplorables‚Äù o ‚Äúaccesibles‚Äù. El proyecto de ley requiere que los datos sean recopilados y reportados al Procurador General. Este conjunto de datos tal como es sirve para ese prop√≥sito. . sobre los datos . . El conjunto de datos est√° compuesto por un archivo .csv de 641.4 MB que contiene 1.8 millones de filas y 143 columnas. Cada parada tiene un DOJ_RECORD_ID √∫nico y cada persona detenida tiene un PERSON_NUMBER. En total, hay 1,708,377 paradas en este conjunto de datos que involucran a 1,800,054 personas. . El conjunto de datos incluye informaci√≥n b√°sica sobre cada parada (como duraci√≥n, hora del d√≠a, ciudad m√°s cercana, nombre de la agencia), informaci√≥n demogr√°fica percibida (raza / etnia, g√©nero, edad, discapacidad), as√≠ como informaci√≥n sobre el motivo de detener, buscar, incautar, acciones tomadas durante la detenci√≥n, contrabando o evidencia encontrada, etc. Para obtener informaci√≥n m√°s detallada sobre el conjunto de datos, se puede leer el archivo README que reuni√≥ el Departamento de Justicia y el Informe Anual 2020. . üí°Nota: Los datos reflejan la percepci√≥n del oficial. Son datos demogr√°ficos percibidos, es decir, si pareces hispano, te marcan como hispano. Si pareces de 27 a√±os te marcan de 27 a√±os. Este es otro problema para otra ocasi√≥n pero hay que tenerlo en mente cuando trabajamos con estos datos. . Esta es la Ola I de una serie de datos de la Racial and Identity Profiling Act (RIPA) que se lanzar√°n en los a√±os siguientes. Los datos publicados sobre esta ola pertenecen a las 8 agencias de aplicaci√≥n de la ley (LEA, por sus siglas en ingl√©s) m√°s grandes de California (las que emplean a m√°s de 1,000 oficiales). Estas LEA (y su parte de las observaciones totales) son: . Law enforcement agency (LEA) N % . California Highway Patrol‚Ää | ‚Ää1,033,421 | 57.4% | . Los Angeles Police Department | ‚Ää 336,681 | 18.7% | . Los Angeles Sheriff Department ‚Ää | ‚Ää 136,635 | 7.59% | . San Diego Police Department ‚Ää | ‚Ää 89,455 | 4.97% | . San Bernardino Sheriff Department‚Ää | ‚Ää 62,433 | 3.47% | . San Francisco Police Department‚Ää | ‚Ää 56,409 | 3.14% | . Riverside Sheriff Department ‚Ää | ‚Ää 44.505 | 2.47% | . San Diego Sheriff Department ‚Ää | ‚Ää 40,515 | 2.25% | . README | Informe Anual 2020 | . sobre datasette . Datasette es una herramienta para explorar y publicar datos. Ayuda a las personas a tomar datos de cualquier forma o tama√±o y publicarlos como un sitio web interactivo y explorable y la API que lo acompa√±a. Datasette est√° dirigido a periodistas de datos, conservadores de museos, archiveros, gobiernos locales y cualquier otra persona que tenga datos que deseen compartir con el mundo. Es parte de un ecosistema m√°s amplio de herramientas y complementos dedicados a hacer que trabajar con datos estructurados sea lo m√°s productivo posible . - datasette.readthedocs.io . datasette es el motor que impulsa este proyecto. En resumen, toma una base de datos sqlite y crea un sitio web interactivo y explorable y API que lo acompa√±a. Para preparar los datos, utilizamos csvs-to-sqlite, otra herramienta del ecosistema datasette que toma archivos CSV y crea bases de datos sqlite a partir de ellos. . Puedes encontrar ejemplos de datasettes p√∫blicos en el wiki del repositorio en GitHub. Aqu√≠ se encuentra uno siriviendo los conjuntos de datos publicados por FiveThirtyEight (puedes encontrarlos en su repositorio de GitHub): https://fivethirtyeight.datasettes.com/ . La p√°gina principal muestra la licencia y la fuente de la base de datos. Tambi√©n provee acceso r√°pido a algunas de sus tablas. . . Si haces clic en el nombre de la base de datos, proporcionar√° una vista de todas sus tablas y un cuadro de texto para que ejecute consultas SQL personalizadas. Esta es una de las caracter√≠sticas que hace que datasette sea una herramienta tan poderosa. . . Si haces clic en una de las tablas, ver√°s las facetas sugeridas, un bot√≥n de filtro f√°cil de usar, un enlace JSON y CSV que proporcionar√° la tabla como uno de esos formatos (esto significa que puede usar esto como API) y una descripci√≥n de la tabla. En este caso, la descripci√≥n contiene una tabla HTML con encabezados y definiciones. . . Si haces clic en el bot√≥n ‚ÄúView and edit SQL‚Äù, volver√°s a tener acceso a un cuadro de texto para escribir sus propias consultas. . . datasette es parte de un ecosistema de herramientas y plugins. Puedes agregar el plugin datasette-vega (repo) para agregar visualizaciones interactivas de la tabla (hechas con altair) . . As√≠ es como se ve un sitio web b√°sico de datasette pero datasette es altamente personalizable. Tomemos, por ejemplo, el datasette de The Baltimore Sun para que las personas exploren los registros de salarios p√∫blicos que se actualizan anualmente. . . Esto esta corriendo datasette detr√°s de c√°maras pero The Baltimore Sun agrego sus propias plantillas y archivos CSS y JS. Para aprender m√°s sobre como personalizar datasette visita https://datasette.readthedocs.io/en/stable/custom_templates.html . paso, paso a pasito . Esta secci√≥n es una descripci√≥n m√°s t√©cnica del proyecto. La preparaci√≥n y despliegue de datos con datasette es bastante sencillo y se puede dividir en tres fases: . Adquirir los datos | Preparar los datos | Servir los datos | . adquiriendo los datos . Los datos se obtuvieron del sitio web de Open Justice del Departamento de Justicia de California: https://openjustice.doj.ca.gov/data. El sitio web proporciona un enlace para descargar los datos (por lo menos desde 5 de mayo de 2020, cuando lo descargu√© para este proyecto). Para obtener m√°s informaci√≥n sobre los datos en s√≠, puede leer la secci√≥n Acerca de los datos en el repositorio de GitHub del proyecto. . preparando los datos . El CSV original es de mas de 640 MB as√≠ que el primer paso es dividirlo en 15 archivos para que cada uno pueda subirse a GitHub en este repositorio. . import pandas as pd import numpy as np from pathlib import Path THIS_FILE = Path(__file__) THIS_DIR = THIS_FILE.parent EXTERNAL_DATA = THIS_DIR.joinpath(&quot;./../../data/external/&quot;) INTERIM_DATA = THIS_DIR.joinpath(&quot;./../../data/interim/&quot;) FULL_DB = &quot;RIPA Stop Data 2018.csv&quot; data = pd.read_csv(EXTERNAL_DATA.joinpath(FULL_DB)) dfs = np.array_split(data, 15) for idx,df in enumerate(dfs): print(f&quot;Saving dataset #{idx}&quot;) df.to_csv(INTERIM_DATA.joinpath(f&quot;ripa-2018-part-{idx}.csv&quot;), index = False) . - https://github.com/chekos/RIPA-2018-datasette/blob/fea53ce60ae43f0d7b0dd130109a01496f08e20a/src/data/split_data.py#L1-L16 . üí°¬øPor qu√©? Nom√°s por si desaparece misteriosamente de la fuente original‚Ä¶ . Sin embargo, debido a que el conjunto de datos es demasiado grande para servir como una sola tabla (1,8 millones de filas por 143 columnas), tambi√©n se dividi√≥ en tablas m√°s peque√±as. Esto significa que tomamos variables relacionadas (basadas en sus sufijos) y las extrajimos en sus propias tablas. Por ejemplo, variables relacionadas con el g√©nero como G_FULL, G_MALE, G_FEMALE, G_TRANSGENDER_MAN, G_TRANSGENDER_WOMAN,G_GENDER_NONCOMFORMING, y G_MULTIGENDER se extrajeron de la tabla ‚Äúprincipal ‚Äúy se agregaron a una tabla de g√©nero en la base de datos. . Estas se pueden volver a unir a la tabla principal utilizando un UNIQUE_ID asignado a ellas. . Este script es demasiado grande para incluir en este blog sin crear una distracci√≥n inecesaria pero puedes ver las 91 l√≠neas de c√≥digo aqu√≠: https://github.com/chekos/RIPA-2018-datasette/blob/master/src/data/break_down_database.py . A cada observaci√≥n o fila de este conjunto de datos se le asigna un DOJ_RECORD_ID y un PERSON_NUMBER. Estos son exclusivos de la parada y las personas detenidas, respectivamente. Esto significa que podr√≠amos combinarlos para crear un ‚ÄòID √öNICO‚Äô para cada fila que podr√≠amos usar para unir tablas. Sin embargo, esto termina siendo una cadena de 22 caracteres que es innecesariamente grande. Para facilitar las cosas, a cada fila se le asigna una identificaci√≥n num√©rica que comienza en 1,000,000. Comenzar en un mill√≥n es completamente arbitrario, podr√≠amos haber comenzado en cero, pero debido a que hay 1.8 millones de filas, tomamos la decisi√≥n de que cada identificaci√≥n num√©rica sea de siete d√≠gitos. Este UNIQUE_ID num√©rico nos permite unir tablas juntas y no es una gran adici√≥n a la base de datos en t√©rminos de memoria. . Una vez que se ha creado este ID_√öNICO, podemos extraer columnas de la tabla ‚Äúprincipal‚Äù en sus propias tablas y guardarlas como archivos CSV individuales con la certeza de que cada observaci√≥n puede coincidir en estas nuevas tablas. . Luego usamos csvs-to-sqlite para crear una base de datos sqlite donde cada CSV es una tabla. En este paso, tambi√©n incluimos el archivo Appendix B Table 3.csv obtenido tambi√©n del sitio web del DOJ y cualquier otra tabla complementaria que podr√≠amos haber creado para acompa√±ar el conjunto de datos. . csvs-to-sqlite data/processed/*.csv &quot;data/external/Appendix B Table 3.csv&quot; datasette/ripa-2018-db.db . sirviendo los datos . Despu√©s de preparar los datos y crear una base de datos sqlite usamos datasette para servirlos como un sitio web interactivo y su API. Esto es tan f√°cil como ejecutar . datasette ripa-2018.db . Pero para este proyecto personalizamos nuestro datasette un poco. . Incluimos un t√Ætulo, una descripci√≥n, la url de la fuente de los datos y archivos CSS y JS. Puedes explorar el archivo metadata.json en el repositorio datasette/metadata.json para ver que m√°s hicimos. . Tambi√©n inclu√≠mos lo que datasette llama canned queries, lo que ser√≠a ‚Äúconsultas almacenadas‚Äù (?). Estas son consultas de SQL inclu√≠das en tu instancia que aparecen al fondo de tu p√°gina principal y tiene su propio URL para facilitar el acceso. Estas consultas las incluimos porque muestran informaci√≥n √∫til y/o interesante de los datos. Algunas de ellas son consultas que computan informaci√≥n especifica publicada en el reporte anual del 2020. . Tambi√©n modificamos algunas plantillas de datasette, especificamente base.html y query.html. La primera fue modificada para incluir metadatos en la etiqueta &lt;head&gt; (descripci√≥n del sitio, por ejemplo, para cuando se comparta un enlace). La segunda fue modificada para incluir un bot√≥n debajo del cuadro de texto donde escribes tus consultas de SQL. Este bot√≥n sirve para facilitar las sugerencias de consultas para el repositorio. Al hacer clic, se abre otra ventana en tu navegador que crea un nuevo issue en github con la consulta de SQL que acabas de ejecutar. . Tambi√©n cambiamos algunas opciones para datasette: . default_page_size:50 - Muestra solo 50 resultados por p√°gina | sql_time_limit_ms:30000 - Un l√≠mite de 30 segundos para ejecutar consultas (es el l√≠mite de tiempo de Heroku) | facet_time_limit_ms:10000 - El tiempo l√≠mite que datasette deber√≠a usar calculando una posible faceta de tu tabla (el default es de 200ms pero nuestro conjunto de datos es tan grande que lo expandimos a 10 segundos) | El c√≥digo por ejecutar es: . datasette ripa-2018-db.db -m metadata.json --extra-options=&quot;--config default_page_size:50 --config sql_time_limit_ms:30000 --config facet_time_limit_ms:10000&quot; . desplegando en heroku . Aqu√≠ hay una descripci√≥n general de alto nivel del proceso . . Hasta ahora, utilizamos un c√≥digo de Python para procesar nuestros datos (creando un UNIQUE_ID y desglosando el conjunto de datos original con 143 columnas en varios archivos .csv m√°s peque√±os) y tambi√©n utilizamos csvs-to-sqlite para construir nuestra base de datos ripa-2018-db.db de esos archivos .csv. Hasta ahora, hemos estado interactuando con nuestro datasette ejecutando el comando datasette ripa-2018-db.db que ejecuta un servidor local. Para que nuestro datasette est√© disponible para el mundo, tenemos que implementarlo en l√≠nea y, por suerte, es muy, muy f√°cil de hacer, ya que datasette ya incluye un comando de publicaci√≥n. . Con datasette puedes publicar directamente a Heroku, Google Cloud Run, o Fly (por lo menos desde la versi√≥n 0.42 que es la que utilizamos en este proyecto). Debido a que tengo experiencia previa en la implementaci√≥n de heroku, me pareci√≥ la m√°s f√°cil de las tres, pero todas son excelentes opciones y la documentaci√≥n es f√°cil de seguir. . Implementar en Heroku fue tan f√°cil como ejecutar casi el mismo comando utilizado para servir nuestro datasette localmente. . datasette publish heroku ripa-2018-db.db --name ripa-2018-db -m metadata.json --extra-options=&quot;--config default_page_size:50 --config sql_time_limit_ms:30000 --config facet_time_limit_ms:10000&quot; . üí° Nota la bandera --name que espcif√≠ca el nombre de nuestra aplicaci√≥n en Heroku. Esto significa que la publicar√° a ripa-2018-db.herokuapp.com y sobreescribir√° cualquier aplicaci√≥n previa ah√≠. . Para ejecutar esto, necesitar√° una cuenta heroku e instalar la herramienta de l√≠nea de comandos de heroku. Datasette (usando el la herramienta de l√≠nea de comandos) te pedir√° que inicies sesi√≥n y abrir√° una ventana del navegador para que lo haga. Despu√©s de iniciar sesi√≥n, se encargar√° del resto. . ¬°Listo! En este punto, hemos publicado con √©xito nuestro datasette en heroku y podemos visitar ripa-2018-db.herokuapp.com o ripa-2018.datasettes.cimarron.io/ . . Este ser√° el final de este proceso para muchas personas. Adquirimos y transformamos con √©xito algunos datos y los desplegamos en la nube para que otros puedan explorar e interactuar con ellos. Tambi√©n hemos incluido una descripci√≥n HTML que describe nuestro datasette para los usuarios e incluso algunas consultas almacenadas para que las personas reproduzcan algunos de los hechos publicados en el Informe Anual 2020. . automatizando todo . Si bien los datos subyacentes que estas publicando no cambian con frecuencia, es posible que desees automatizar la implementaci√≥n de su instancia de datasette por muchas razones. Por ejemplo, cuando comenc√© este proyecto, datasette estaba en la versi√≥n 0.40 y en el momento en que escribo esta publicaci√≥n est√° en la versi√≥n 0.42. Para la mayor√≠a de las versiones no necesitar√°s actualizar tu instancia de datasette, pero la versi√≥n 0.41 inclu√≠a la capacidad de crear p√°ginas personalizadas (changelog). . For example, adding a template file called templates/pages/about.html will result in a new page being served at /about on your instance. . Esto significa que podemos agregar mucho m√°s contexto en nuestra instancia para los usuarios. Tal vez incluya una gu√≠a paso a paso para ayudar a las personas a contribuir al proyecto, otros enlaces √∫tiles o una p√°gina simple para presentarse para que las personas que usan estos datos aprendan un poco m√°s sobre ti. . Tambi√©n es posible que desee incluir m√°s consultas almacenadas o corregir un error ortogr√°fico en su descripci√≥n. Cualquiera sea la raz√≥n, la implementaci√≥n autom√°tica es f√°cil de lograr. En definitiva, todo lo que necesita es ejecutar datasette heroku publishing, que es un caso de uso perfecto para las GitHub Actions. . üö® Advertencia / Nota: Esta pr√≥xima parte se vuelve mucho m√°s t√©cnica r√°pidamente. GitHub Actions es un tema m√°s avanzado. Si no necesitas / deseas actualizar su instancia de datasette recientemente implementada regularmente, no recomendar√≠a pensar en las GitHub Actions por el momento. . . GitHub Actions . GitHub Actions help you automate your software development workflows in the same place you store code and collaborate on pull requests and issues. You can write individual tasks, called actions, and combine them to create a custom workflow. Workflows are custom automated processes that you can set up in your repository to build, test, package, release, or deploy any code project on GitHub. - https://help.github.com/en/actions/getting-started-with-github-actions/about-github-actions . No entraremos muy a fondo en las acciones de GitHub en esta publicaci√≥n. Lo que necesitas saber es que con las GitHub Actions puede ejecutar tareas en paralelo o en secuencia, y estos se desencadenan por eventos de GitHub como empujar tu c√≥digo a una branch, abrir una pull request, comentar en un issue o una combinaci√≥n de muchos. . Estos existen en sus repositorios de GitHub en .github/workflows/ generalmente en forma de archivos yaml. La estructura b√°sica es la siguiente. . name: Example of simple GitHub Action on: push: branches: [master] jobs: say-hi: runs-on: ubuntu-latest steps: - name: Echo Hello World run: | echo &quot;Hello World!&quot; . Esta acci√≥n se activar√° cada vez que haya un impulso en su rama maestra y ejecutar√° el trabajo say-hi que se ejecuta en una m√°quina virtual (VM) ubuntu disponible. En esa VM ejecutar√° el c√≥digo echo &quot;Hello World!&quot; . Puede ver los resultados / registros de sus acciones en la pesta√±a Actions de tu repositorio. . . Podr√≠as cambiar facilmente echo &quot;Hello World!&quot; con . datasette publish heroku ripa-2018-db.db --name ripa-2018-db -m metadata.json --extra-options=&quot;--config default_page_size:50 --config sql_time_limit_ms:30000 --config facet_time_limit_ms:10000&quot; . o mejor a√∫n puedes guardar tu c√≥digo en un script de bash llamado heroku_deploy.sh y ejecutar sh heroku_deploy.sh como ejecutar√≠as echo &quot;Hello World!&quot; . Afortunadamente, Heroku ya est√° instalado en nuestro corredor de la GitHub Action (ubuntu-latest), por lo que todo lo que tenemos que hacer es iniciar sesi√≥n, instalar el complemento heroku-builds y ejecutar nuestro script heroku_deploy.sh. . üí° Aprend√≠ sobre la necesidad de instalar heroku-builds despu√©s de que las acciones de GitHub fallaran un par de veces. No estoy seguro de que est√© documentado en datasette o en la documentaci√≥n de GitHub Actions. . Hasta ahora, nuestra GitHub Action se ve as√≠: . name: Example of simple GitHub Action on: push: branches: [master] jobs: publish-to-heroku: runs-on: ubuntu-latest steps: - name: Publish to Heroku run: | heroku container:login &amp;&amp; heroku plugins:install heroku-builds &amp;&amp; sh heroku_deploy.sh . Esto, sin embargo, a√∫n no est√° listo para funcionar. Para automatizar el inicio de sesi√≥n, debe incluir la variable de entorno HEROKU_API_KEY. Esto es posible mediante el uso de GitHub Secrets. . Para crear tu API key necesitas tener instalada la herramienta de heroku para la l√≠nea de comando localmente. Ejecuta el comando heroku authorizations:create y a√±adela a tu repositorio en Settings &gt; Secrets. Ll√°malo HEROKU_API_KEY. . Para que su GitHub Action tenga acceso a ella, debe agregar la siguiente l√≠nea a su archivo yaml . name: Example of simple GitHub Action on: push: branches: [master] jobs: publish-to-heroku: runs-on: ubuntu-latest steps: - name: Publish to Heroku env: $ run: | heroku container:login &amp;&amp; heroku plugins:install heroku-builds &amp;&amp; sh heroku_deploy.sh . ¬°Listo! Ahora, cada vez que empujes c√≥digo a tu repositorio en la branch ‚Äúmaster‚Äù, implementar√° una nueva versi√≥n de tu datasette. Esto significa que puede actualizar su metadata.json, por ejemplo, con una nueva consulta almacenada o agregar una nueva p√°gina a tus templates/pages. . Para este proyecto, tengo algunos pasos adicionales (pasos de GitHub Action, es decir) que procesan los datos, construyen la base de datos sqlite3 y la despliegan en heroku cada vez que hay un nuevo ‚Äúcommit‚Äù en ‚Äúmaster‚Äù. Tambi√©n incluyo un par de scripts de Python que leen un archivo yaml separado donde tengo todas las consultas almacenadas y las agrego a un updated_metadata.json. Esto es para mantener las cosas un poco m√°s limpias, cada consulta almacenada tiene un t√≠tulo, html_description y una larga consulta SQL; Es m√°s f√°cil de mantener como un archivo separado, en mi opini√≥n. . Puedes ver el archivo yaml de mi GitHub Action aqu√≠ https://github.com/chekos/RIPA-2018-datasette/blob/master/.github/workflows/main.yml . bonus . using github issues to suggest¬†queries . Si visitas y ejecutas una consulta, notar√°s el bot√≥n **Submit in GitHub**. . . Esto se hace mediante un ajuste de la plantilla query.html. Datasette usa jinja2 y en realidad est√° pasando la consulta SQL como un par√°metro de consulta de URL, lo que significa que puede acceder a √©l usando request.query_string.split(&#39;sql =&#39;)[- 1] . Ya que tienes acceso a la consulta es facil crear un enlace directo a un issue nuevo en tu repo . {% set link_to_new_issue = &quot;&lt;https://GitHub.com/&gt;&lt;YOUR_USERNAME&gt;/&lt;YOUR_REPO&gt;/issues/new?title=Query+suggestion&amp;labels=suggestion&amp;body=&quot; + &lt;QUERY_FOR_ISSUE&gt; %} . As√≠ se ve en query.html . ... {% set query_for_issue = &quot;%23+title%0A%0A%23+query%0A%60%60%60sql%0A&quot; + request.query_string.split(&#39;sql=&#39;)[-1] + &quot;%0A%60%60%60&quot; %} {% set link_to_new_issue = &quot;https://GitHub.com/chekos/ripa-2018-datasette/issues/new?title=Query+suggestion&amp;labels=suggestion&amp;body=&quot; + query_for_issue %} &lt;p&gt; &lt;button id=&quot;sql-format&quot; type=&quot;button&quot; hidden&gt;Format SQL&lt;/button&gt; &lt;input type=&quot;submit&quot; value=&quot;Run SQL&quot;&gt; &lt;button type=&quot;button&quot; class=&quot;btn btn-secondary btn-sm&quot;&gt;&lt;a href=&quot;{{ link_to_new_issue }}&quot; target=&quot;_blank&quot;&gt;Submit on &lt;i class=&quot;fab fa-github&quot;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/button&gt; &lt;/p&gt; ... . cuanto tiempo tom√≥ . Todo el proceso tom√≥ alrededor de 20 horas en total, distribuidas en 3 a 4 semanas. La mayor parte estaba planeando y orquestando todo el proceso automatizado usando GitHub Actions. Espero que este tutorial te ayude a ahorrar esas horas extra. Al igual que cualquier otro proyecto de datos, es mejor pasar un tiempo por adelantado pensando y planeando detalladamente cada paso del proceso. . llamado a la acci√≥n . El mundo ha cambiado. La idea de este proyecto se me ocurri√≥ a mediados de abril y no cre√© el repositorio de GitHub hasta finales de mes. No comenc√© a escribir este tutorial hasta la segunda mitad de mayo. Todo ha cambiado desde entonces. . Todos los d√≠as en las redes sociales vemos videos y leemos historias sobre autoridades que abusan de su poder y ejercen violencia contra personas inocentes. No son todos, pero no deber√≠a ser ninguno de ellos. . Creo en los datos como una herramienta para el cambio, para la rendici√≥n de cuentas, para la transparencia. El mundo ha cambiado para siempre por la tecnolog√≠a y los datos. Como tecn√≥logos, nerds de datos, expertos en pol√≠ticas o cualquier etiqueta autoascrita que elijan, tu que me lees, tenemos la responsabilidad de que nuestros pares usen nuestras habilidades, nuestras poderosas herramientas, nuestro conocimiento para hacer del mundo un lugar mejor porque podemos. . Te pido que pienses en el papel que desempe√±as. ¬øHar√°s del mundo un lugar mejor con las herramientas que est√°s construyendo, por favor? . Te dejo con el siguiente pensamiento de Data4BlackLives: . Data for Black Lives es un movimiento de activistas, organizadores y matem√°ticos comprometidos con la misi√≥n de utilizar la ciencia de datos para crear un cambio concreto y medible en la vida de las personas negras. Desde el advenimiento de la inform√°tica, la big data y los algoritmos han penetrado pr√°cticamente todos los aspectos de nuestra vida social y econ√≥mica. Estos nuevos sistemas de datos tienen un enorme potencial para empoderar a las comunidades de color. Herramientas como el modelado estad√≠stico, la visualizaci√≥n de datos y el abastecimiento p√∫blico, en las manos correctas, son instrumentos poderosos para combatir el sesgo, construir movimientos progresivos y promover el compromiso c√≠vico. Pero la historia cuenta una historia diferente, una en la que los datos se utilizan con demasiada frecuencia como un instrumento de opresi√≥n, que refuerza la desigualdad y perpet√∫a la injusticia. Redlining fue una empresa basada en datos que result√≥ en la exclusi√≥n sistem√°tica de las comunidades negras de los servicios financieros clave. Las tendencias m√°s recientes como la vigilancia predictiva, las sentencias basadas en el riesgo y los pr√©stamos abusivos son variaciones preocupantes sobre el mismo tema. Hoy, la discriminaci√≥n es una empresa de alta tecnolog√≠a. - d4bl.org . . Este art√≠culo fue publicado originalmente en ingl√©s en la publicaci√≥n Towards Data Science, Making open data more accessible with datasette .",
            "url": "https://chekos.dev/datasette/datos%20abiertos/2020/06/25/haciendo-datos-abiertos-mas-accesibles-con-datasette/",
            "relUrl": "/datasette/datos%20abiertos/2020/06/25/haciendo-datos-abiertos-mas-accesibles-con-datasette/",
            "date": " ‚Ä¢ Jun 25, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Alem√°n: Alemalacra Alemalandro Alemaliya",
            "content": "Alem√°n: Alemalacra Alemalandro Alemaliya . La historia . Antes de crear tacosdedatos intent√© crear un blog de Hip Hop Latino-americano donde iba a analizar de una manera cuantitativa la calidad de discos y canciones. . No lleg√≥ muy lejos, era en Square space y no era tan f√°cil escribir notas. Bueno, no tan f√°cil como lo es ahora que aprend√≠ de blogs y sitios est√°ticos en GitHub. Aqu√≠ puedo escribir todo en mi celular y copiarlo a un archivo markdown directamente en GitHub y voil√† tengo un blog. . El que haya fallado elblogdehiphop no significa que mi amor por el Hip Hop haya disminuido ni siquiera un poco. Seg√∫n Spotify, mis artistas m√°s reproducidos son, en orden: . Alem√°n | La Plebada | Gera MX | Remik Gonz√°lez | West Gold | Esta nota es sobre el n√∫mero uno: Alem√°n. . Alem√°n ha sido uno de mis artistas favoritos desde la primera vez que lo escuch√©. En aquellos tiempos yo no sab√≠a lo que se de Hip Hop pero muy dentro de mi sent√≠a que Alem√°n era excelente en lo que hac√≠a. No sab√≠a que era eso todav√≠a pero sab√≠a que √©l era uno de los grandes. . Hoy en d√≠a, uso la palabra flow para describir lo que diferencia Alem√°n de los dem√°s. . El genio de Alem√°n es saber que decir, cuando decirlo y como decirlo. No suena repetitivo, no suena forzado. En otra nota hablaremos m√°s del flow de Aleman. . Yo creo no soy el √∫nico que piensa esto de Alem√°n porque es de los pocos raperos mexicanos con m√°s de un mill√≥n de seguidores en Spotify. . El que sea popular ahora no significa que siempre lo fue. Alem√°n comenz√≥ en la Mexamafia como Gera MX. Un grupo conocido por su calidad hardcore y underground. Curiosamente ambos, Alem√°n y Gera MX son artistas muy exitosos el d√≠a de hoy y aunque se adentren en el mundo del trap y sonidos m√°s populares no pierden el respeto como exponentes del Hip Hop mexicano. . En mi mente existen estas conexiones entre todos estos artistas. Alem√°n con Gera MX por la Mexamafia. Pero Alem√°n ahora est√° en la Homegrown con La Banda Bast√∂n, Yoga Fire, Fntxy, Cozy Cuz, Mike D√≠az, Dee. Dee es parte de Hood P con MOF. Mike D√≠az es Neverdie con el Eptos. La Banda Bast√∂n es Vieja Guardia. Al Gera lo relaciono con Charles Ans pero Charles es Anestesia. Charles Ans tiene rolas con Taxi Dee (el nombre que Fntxy usa cuando produce). Fntxy ahora tiene el grupo La Plebada junto a Cozy Cuz quien va Bobby Bass cuando produce. Bobby Bass comenz√≥ a agarrar m√°s tracci√≥n cuando comenz√≥ a trabajar con Alem√°n. . Todos se conectan. En mi mente, por lo menos. Quer√≠a saber si los datos respaldaban mis pensamientos. . Hace unas semanas encontr√© esta herramienta: http://labs.polsys.net/playground/spotify/ . La herramienta utiliza la API de Spotify para crear una red de artistas relacionados hasta dos niveles de separaci√≥n. Es decir, cuando yo escribo Alem√°n en la caja de texto la herramienta va y busca todos los artistas relacionados a Alem√°n (nivel uno) y tambi√©n busca los artistas relacionados esos artistas (nivel dos). . . Note: Si te interesar√≠a saber m√°s de la metodolog√≠a detr√°s de esta herramienta d√©jame un comentario en este post ü§ìüéß As√≠ se ve la red de Alem√°n . La herramienta tiene la opci√≥n de descargar los datos. Uno de los atributos de esos datos son las IDs √∫nicas que Spotify le asigna a cada artista. Con estas IDs puedes utilizar la API de Spotify para obtener m√°s informaci√≥n de cada artista como su √≠ndice de popularidad, cu√°ntos seguidores tienen, sus canciones m√°s populares y mucho m√°s. . Justo eso fue lo que hice para crear una visualizaci√≥n diferente. Sabiendo que el ‚Äúuniverso‚Äù de mis datos es artistas relacionados a Alem√°n hasta dos niveles de separaci√≥n puedo hacer preguntas como: . ¬øc√≥mo se compara la popularidad de Alem√°n con la de artistas relacionados? | ¬øde qu√© g√©neros musicales vienen √©stos artistas? | ¬øcu√°ntos artistas relacionados a Alem√°n tienen m√°s de un mill√≥n de seguidores? | M√°s que todo esto, quer√≠a una manera f√°cil de explorar estos datos. . El resultado fue este Observable Notebook: https://observablehq.com/@chekos/aleman-beeswarm-plot-using-spotify-data . . Tip: Esto se ve mejor en el Notebook y en tu computadora. No lo he &#39;optimizado&#39; para m√≥vil. La visualizaci√≥n . Hay solo 5 artistas (+ Alem√°n) con m√°s de un mill√≥n de seguidores en Spotify: . Cartel de Santa 3.94M | El Komander 1.46M | Molotov 1.42M | Pante√≥n Rococo 1.28M | Beret 1.15M | Alem√°n 1.08M | Muchos son artistas de rap y hip-hop pero tambi√©n hay artistas de pop, rock en espa√±ol, reggea y ska. . Alem√°n est√° entre los m√°s populares de este universo lo cual me estoy tomando la libertad de etiquetar como positivo. De alguna manera, este artista underground que lleg√≥ a esta altura le est√° abriendo la puerta a todos estos dem√°s artistas con menos popularidad. M√≠nimo, Spotify los identifica como artistas relacionados y tal vez aparezcan en una de esas listas de reproducci√≥n automatizadas juntos ü§∑üèª‚Äç‚ôÇÔ∏è .",
            "url": "https://chekos.dev/hip%20hop/aleman/2020/02/15/aleman-alemaniaco-alemalandro-alemaliya/",
            "relUrl": "/hip%20hop/aleman/2020/02/15/aleman-alemaniaco-alemalandro-alemaliya/",
            "date": " ‚Ä¢ Feb 15, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Ya se fueron las nieves de enero",
            "content": "Ya se fueron las nieves de enero . Cuando todo va mal es cuando m√°s me motivo / Respiro mejor en √©ste ambiente nocivo / Esto para el rap es lo m√°s nutritivo / Es todo lo que soy por eso escribo. . Nocivos, Faruz Feet &amp; Proof | . ¬øPor qu√© har√≠a otro blog? . Si me sigues en las redes sociales tal vez conozcas de la comunidad tacosdedatos que comenc√© a inicios del 2019. La idea de tacosdedatos es crear contenido en espa√±ol. Contenido sobre el an√°lisis y la visualizaci√≥n de datos. Contenido sobre mejores pr√°cticas. Contenido sobre t√©cnicas y tendencias en el mundo de la tecnolog√≠a que parecen quedarse en el mundo angloparlante por mucho tiempo antes de llegar al mundo hispanohablante. . tacosdedatos ha crecido mucho en este √∫ltimo a√±o. Tenemos m√°s de 3,400 seguidores en Twitter, miles de visitas al sitio web y lo m√°s importante: muchas personas participando en la conversaci√≥n del an√°lisis / visualizaci√≥n de datos en espa√±ol. Cada semana recibo por lo menos un mensaje directo en twitter o un correo con dudas de c√≥mo comenzar en Python o R o como resolver X problema con su c√≥digo o como crear X visualizaci√≥n. Tal vez a muchas personas no les impresione hablar con un extra√±o pidiendo ayuda pero eso es lo que me llena de ganas de seguir haciendo lo que hago. . Entonces, si a tacosdedatos le va tan bien, ¬øpor qu√© crear otro blog? La verdad es que aunque existen ya muchos ‚Äúproductos‚Äù - el bolet√≠n, el canal de Youtube, la academia, los dos podcast, el sitio web - todav√≠a existen muchas cosas de las que me gustar√≠a escribir y simplemente no ‚Äúcaben‚Äù en la filosof√≠a de tacosdedatos. Por ejemplo, yo antes de crear tacosdedatos hab√≠a creado elblogdehiphop d√≥nde hac√≠a an√°lisis (si eso le podemos llamar a lo que hac√≠a jaja) y visualizaciones de datos sobre el Hip Hop latino-americano. Si siguen a Noisey en Espa√±ol tal vez habr√°n visto esta nota d√≥nde ‚Äúaporte‚Äù una humilde opini√≥n de las mejores canciones de 2017: Las mejores canciones del rap mexicano en 2017. . Este tipo de proyectos no son did√°cticos por naturaleza; algo que me gustar√≠a mantener en todo relacionado a tacosdedatos. . La segunda raz√≥n es que si yo escribiera todas estas ideas en tacosdedatos.com saturar√≠a el sitio y la idea es crear una comunidad. Quiero que m√°s personas escriban para tacosdedatos no que sea mi blog personal - es un foro. . Y la tercera raz√≥n es que yo promuevo mucho el que todxs tengamos un sitio/blog personal para mostrar el trabajo que hacemos. Ya seas una dise√±adora, una programadora, una analista - todxs necesitamos un ‚Äúportafolio‚Äù que podamos presentar lo que hacemos de una manera pr√°ctica y accesible. Este blog servir√° como ejemplo. . Como lo hice . La raz√≥n principal por la que decid√≠ utilizar este m√©todo de publicaci√≥n es la facilidad. Fast.ai creo un repositorio de GitHub que sirve como plantilla. Se encuentra aqu√≠. . Puedes crear un repositorio apartir de el suyo que ya est√° listo para ser publicado en GitHub Pages de manera gratuita y f√°cil. Lo √∫nico que necesitas es una cuenta de GitHub y crear un repositorio llamado &lt;USUARIO&gt;.github.io d√≥nde USUARIO es tu nombre de usuario utilizando este enlace: https://GitHub.com/fastai/fast_template/generate/. Es decir, si creas una cuenta de GitHub con el usuario ‚Äúpapichulo‚Äù solo tienes que hacer clic en ese enlace y crear papichulo.github.io y ya. . GitHub sabe que cuando creas un repositorio siguiendo esas convenciones es porque quieres usar GitHub Pages y el repositorio base incluye todos los archivos necesarios para configurar tu blog y publicarlo sin ning√∫n problema. . Esto es esencial porque como les mencion√© yo ya tengo muchos proyectos que debo mantener. Uno m√°s, por m√°s importante que sea, ser√≠a costar√≠a mucho trabajo mantener. . Usando este m√©todo me tard√© 3 minutos en crear este blog y para agregar contenido solo toma agregar archivos .md a la carpeta _posts/. . De hecho, este post lo escrib√≠ en mi tel√©fono en camino a casa. Esa es la facilidad que necesito para tener otro blog. . Esa es la facilidad que t√∫ tienes si decides comenzar uno tambi√©n. . tonx que de que o qu√© . ¬øSobre qu√© voy a escribir en este blog? Honestamente, no lo s√©. Probablemente ser√° un lugar para presumir alguna visualizaci√≥n que haya hecho, o un an√°lisis de mis versos de rap mexicano favoritos (¬øhaz escuchado Verbal Big Bang del Anexo Leiruk? ¬øSuelo so√±ar, correr y tropezar de Gera MX? ¬øUnorthodox (DJ kingklan remix) de Eptos y Buffon?) o tal vez sobre tendencias en el mundo de la tecnolog√≠a o cosas que me enojen del mundo (como lo que est√° sucediendo con el libro American Dirt en estados unidos). . Lo m√°s probable es que haga peque√±os blogs sobre c√≥mo logr√© hacer algo en Python o R o d3. Micro tutoriales. . Necesitamos m√°s de eso en espa√±ol. Existe stack overflow y existen blogs en ingl√©s pero para los que apenas vamos comenzando en ciertas cosas el tener que aprender algo nuevo y aprenderlo en ingl√©s es un reto m√°s imponente de lo que deber√≠a ser. . En el anuncio de fast_template comparten la liga a un blog que la cofundadora de fast.ai escribi√≥ sobre bloguear. En √©l menciona, entre otras cosas, que la mejor persona para ense√±arle a alguien que est√° un paso detr√°s c√≥mo hacer algo eres t√∫. Es decir, si acabas de aprender c√≥mo funcionan las geoms en ggplot2 o como aplicar las etiquetas de los datos a un dataframe de pandas - t√∫ eres la mejor persona para ense√±arle c√≥mo hacerlo a alguien que est√° justo queriendo aprender a hacer eso. Por eso, no importa tu nivel de experiencia, t√∫ deber√≠as tener un blog. . Te recomiendo fast_template c√≥mo lo estoy haciendo yo aqu√≠ pero tambi√©n existe Medium.com y hasta tacosdedatos.com. Lo importante es comenzar. . conclusiones . Todxs tenemos algo que decir. . Tal vez pienses que como principiante no hay raz√≥n por qu√© tener un blog - yo opino lo contrario. Necesitamos m√°s contenido nivel principiante e intermedio en espa√±ol. . Tal vez pienses que nadie lo va a leer. Te prometo que m√°s personas de las que crees est√°n esperando lo que vas a escribir. . Tal vez pienses es muy dif√≠cil. Si usas la plantilla fast_template est√°s a unos cuantos clics de tener tu blog - no necesitas saber Git no GitHub ni HTML ni javascript ni nada. . Comienza tu blog. . Si necesitas ayuda m√°ndame un mensaje por twitter, Instagram, Facebook o hasta un correo üì®. .",
            "url": "https://chekos.dev/personal/2020/02/10/las-nieves-de-enero/",
            "relUrl": "/personal/2020/02/10/las-nieves-de-enero/",
            "date": " ‚Ä¢ Feb 10, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Sobre chekos.dev",
          "content": "¬°Hola! Mi nombre es Sergio S√°nchez Zavala - creador de @tacosdedatos. Aunque ya tenemos un mont√≥n de productos como el sitio web, el podcast tacosdedatos, el podcast, el podcast Quail data (y su sitio tacosdedatos.fm), el bolet√≠n, el canal de youtube y hasta ‚Äúla academia‚Äù - aunque todo eso existe hay cosas que me gustar√≠a documentar que simplemente no cuadran con la filosof√≠a de tacosdedatos. . Para eso existe este blog - . . Yo tengo mi sitio personal en soyserg.io en el que publico informaci√≥n de lo que hago - charlas, talleres, etc. Pero para un blog blog, para eso es . . Este blog esta creado con el fast_template de fast.ai. Tiene github actions para tomar tus jupyter notebooks y crear archivos markdown para publicar en GitHub Pages sin ning√∫n problema. Tambi√©n esta dise√±ado para ser f√°cil de usar incluso si no sabes de Git/GitHub. Para m√°s informaci√≥n visita su blog aqu√≠. . . Tambi√©n tengo una newsletter ahora .",
          "url": "https://chekos.dev/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://chekos.dev/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}