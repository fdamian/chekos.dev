---
layout: post
title: "Haciendo datos abiertos m√°s accesibles con datasette"
description: "Proporcionar los datos es el primer paso, pero para que sean utilizables y accesibles para la mayor√≠a de las personas, no podemos simplemente publicar datos."
toc: true
comments: true
image: images/posts_imgs/datasette-blog.jpeg
show_image: true
categories: [datasette, datos abiertos]
---

California recientemente liber√≥ datos sobre las detenciones hechas por oficiales de las 8 agencias m√°s grandes del estado. Estos datos cubren los meses de julio a diciembre del 2018. Esta fue la primera ola de divulgaci√≥n de datos que entrar√° en vigencia en los a√±os siguientes. Los datos cubrieron m√°s de 1.8 millones de paradas en todo el estado. Si bien este es un paso en la direcci√≥n correcta, un solo archivo `.csv` de alrededor de 640 megabytes con m√°s de 1.8 millones de filas y m√°s de 140 columnas podr√≠a ser intimidante para algunas personas que se beneficiar√≠an de la exploraci√≥n de estos datos: l√≠deres locales, periodistas, activistas y organizadores, por nombrar algunos.

El tercer principio de la [carta internacional de datos abiertos](https://opendatacharter.net/principles-es/) es que los datos deben ser accesibles y utilizables.

Proporcionar los datos es el primer paso, pero para que sean utilizables y accesibles para la mayor√≠a de las personas, no podemos simplemente publicar datos; debemos tener en cuenta la experiencia del usuario y desarrollar formas de facilitar la exploraci√≥n y el uso de dichos datos.

En este tutorial, comparto una forma de hacerlo: usando **datasette**, una _"herramienta de Python para explorar y publicar datos. Ayuda a las personas a tomar datos de cualquier forma o tama√±o y publicarlos como un sitio web interactivo y explorable y con una API acompa√±ante"._

Tomamos los datos de detenci√≥n policial lanzados recientemente por California, los limpiamos y transformamos a como sea necesario, y desplegamos una instancia de **datasette** en Heroku.

## pr√≥logo  

La idea de explorar este conjunto de datos vino al escuchar el episodio del 3 de marzo del 2020 del podcast **Pod Save the People**. (minuto 4:12)

<iframe src="https://open.spotify.com/embed-podcast/episode/1mp0laka9Hkw5jUboBl0Sm" width="100%" height="232" frameborder="0" allowtransparency="true" allow="encrypted-media"></iframe>

donde mencionan un art√≠culo de _The Appeal_:

> _Boudin will announce a second directive today, also reviewed by The Appeal, on what are known as pre-textual stops, in which an officer stops someone for a minor offense or infraction, such as a traffic violation, in order to conduct an unrelated search for items like guns or drugs. According to the new policy, the DA's office will not prosecute possession of contraband cases when the contraband was collected as a result of an infraction-related stop, "where there is no other articulable suspicion of criminal activity." Any deviations from the policy should be made in writing and require approval from the DA or a chief of the criminal division. Additionally, the ban includes cases in which a person consented to a search "because of the long-standing and documented racial and ethnic disparities in law enforcement requests for consent to search," according to the directive._ <br> - <https://theappeal.org/san-francisco-da-to-announce-sweeping-changes-on-sentencing-policy-and-police-stops/>

En el episodio, Sam Sinyangwe menciona que las personas negras y de color est√°n siendo detenidas y registradas a tasas m√°s altas, y que muchos de estos registros son lo que se conocen como 'registros consensuales', lo que significa que la polic√≠a no informa ninguna justificaci√≥n para registrar a la persona mas que preguntar a esa persona si puede registrarlos y que la persona presuntamente da su consentimiento.

Esta gran disparidad racial es desgarradora pero no es sorpresa.

Cuando comenc√© a trabajar con el conjunto de datos me pareci√≥ un tanto √≠ncomodo, no era f√°cil. El tama√±o del conjunto de los datos hace que sea d√≠ficil trabajar con √©l para aquellas personas que no analizan datos program√°ticamente; ya sea con recursos pagados como stata y sas o gratuitos como python y R. 

Esta informaci√≥n no esta dise√±ada para ser explorada f√°cilmente pero existen herramientas que pueden ayudar con eso.

> üí°**Nota**: AB-953, el proyecto de ley que requiere que las agencias reporten todos los datos sobre paradas al Procurador General, no requiere que los datos sean f√°cilmente "explorables" o "accesibles". El proyecto de ley requiere que los datos sean recopilados y reportados al Procurador General. Este conjunto de datos tal como es sirve para ese prop√≥sito.

## sobre los datos

![**source:** <https://openjustice.doj.ca.gov/data>](https://cdn-images-1.medium.com/max/800/1*tDeVEZl_Dbe0DdwkCMPAKQ.png) 

El conjunto de datos est√° compuesto por un archivo `.csv` de 641.4 MB que contiene 1.8 millones de filas y 143 columnas. Cada parada tiene un `DOJ_RECORD_ID` √∫nico y cada persona detenida tiene un `PERSON_NUMBER`. En total, hay 1,708,377 paradas en este conjunto de datos que involucran a 1,800,054 personas.

El conjunto de datos incluye informaci√≥n b√°sica sobre cada parada (como duraci√≥n, hora del d√≠a, ciudad m√°s cercana, nombre de la agencia), **informaci√≥n demogr√°fica percibida** (raza / etnia, g√©nero, edad, discapacidad), as√≠ como informaci√≥n sobre el motivo de detener, buscar, incautar, acciones tomadas durante la detenci√≥n, contrabando o evidencia encontrada, etc. Para obtener informaci√≥n m√°s detallada sobre el conjunto de datos, se puede leer el archivo README que reuni√≥ el Departamento de Justicia y el Informe Anual 2020.

> üí°**Nota**: Los datos reflejan la percepci√≥n del oficial. Son datos demogr√°ficos _percibidos_, es decir, si pareces hispano, te marcan como hispano. Si pareces de 27 a√±os te marcan de 27 a√±os. Este es otro problema para otra ocasi√≥n pero hay que tenerlo en mente cuando trabajamos con estos datos.

Esta es la Ola I de una serie de datos de la _Racial and Identity Profiling Act_ (RIPA) que se lanzar√°n en los a√±os siguientes. Los datos publicados sobre esta ola pertenecen a las 8 agencias de aplicaci√≥n de la ley (LEA, por sus siglas en ingl√©s) m√°s grandes de California (las que emplean a m√°s de 1,000 oficiales). Estas LEA (y su parte de las observaciones totales) son:

|   _Law enforcement agency_ (LEA)  |     N     |    %   |
|:----------------------------------|:---------:|:------:|
|     California Highway Patrol‚Ää    |‚Ää1,033,421 | 57.4%  |
|    Los Angeles Police Department  | ‚Ää 336,681 | 18.7%  |
|  Los Angeles Sheriff Department  ‚Ää|‚Ää  136,635 |  7.59% | 
|     San Diego Police Department  ‚Ää|‚Ää   89,455 |  4.97% |
| San Bernardino Sheriff Department‚Ää|‚Ää   62,433 |  3.47% |
|   San Francisco Police Department‚Ää|‚Ää   56,409 |  3.14% |
|    Riverside Sheriff Department  ‚Ää|‚Ää   44.505 |  2.47% |
|    San Diego Sheriff Department  ‚Ää|‚Ää   40,515 |  2.25% |

[README](https://data-openjustice.doj.ca.gov/sites/default/files/dataset/2020-01/RIPA%20Dataset%20Read%20Me%2020200106.pdf) | [Informe Anual 2020](https://oag.ca.gov/sites/all/files/agweb/pdfs/ripa/ripa-board-report-2020.pdf) 

## sobre datasette

> **Datasette** _es una herramienta para explorar y publicar datos. Ayuda a las personas a tomar datos de cualquier forma o tama√±o y publicarlos como un sitio web interactivo y explorable y la API que lo acompa√±a. **Datasette** est√° dirigido a periodistas de datos, conservadores de museos, archiveros, gobiernos locales y cualquier otra persona que tenga datos que deseen compartir con el mundo. Es parte de un ecosistema m√°s amplio de herramientas y complementos dedicados a hacer que trabajar con datos estructurados sea lo m√°s productivo posible ._ <br> - [_datasette.readthedocs.io_](https://datasette.readthedocs.io/)

**datasette** es el motor que impulsa este proyecto. En resumen, toma una base de datos sqlite y crea un _sitio web interactivo y explorable y API que lo acompa√±a_. Para preparar los datos, utilizamos `csvs-to-sqlite`, otra herramienta del ecosistema **datasette** que toma archivos CSV y crea bases de datos sqlite a partir de ellos.

Puedes encontrar ejemplos de **datasette**s p√∫blicos en el [wiki del repositorio en GitHub](https://github.com/simonw/datasette/wiki/Datasettes). Aqu√≠ se encuentra uno siriviendo los conjuntos de datos publicados por FiveThirtyEight (puedes encontrarlos en su [repositorio de GitHub](https://github.com/fivethirtyeight/data)): <https://fivethirtyeight.datasettes.com/>

La p√°gina principal muestra la licencia y la fuente de la base de datos. Tambi√©n provee acceso r√°pido a algunas de sus tablas.

![](https://cdn-images-1.medium.com/max/800/1*y1-FwGc2JhX_1ocHz7TXBA.png) 

Si haces clic en el nombre de la base de datos, proporcionar√° una vista de todas sus tablas y un cuadro de texto para que ejecute consultas SQL personalizadas. Esta es una de las caracter√≠sticas que hace que **datasette** sea una herramienta tan poderosa.

![](https://cdn-images-1.medium.com/max/800/1*7QZ3wF9k2TbMd79dauftng.png) 

Si haces clic en una de las tablas, ver√°s las facetas sugeridas, un bot√≥n de filtro f√°cil de usar, un enlace JSON y CSV que proporcionar√° la tabla como uno de esos formatos (esto significa que puede usar esto como API) y una descripci√≥n de la tabla. En este caso, la descripci√≥n contiene una tabla HTML con encabezados y definiciones.

![](https://cdn-images-1.medium.com/max/800/1*QWaKpYg7mgvGcbeI0Kcwmw.png)

Si haces clic en el bot√≥n "View and edit SQL", volver√°s a tener acceso a un cuadro de texto para escribir sus propias consultas.

![](https://cdn-images-1.medium.com/max/800/1*e_nW20lBbSPRAgXLYkgYng.png) 

**datasette** es parte de un ecosistema de herramientas y _plugins_. Puedes agregar el _plugin_ **datasette-vega** ([repo](https://github.com/simonw/datasette-vega)) para agregar visualizaciones interactivas de la tabla (hechas con [altair](https://altair-viz.github.io/))

![](https://cdn-images-1.medium.com/max/800/1*zZ7i8wiP7yBc-cKx7tPW8w.png) 

As√≠ es como se ve un sitio web b√°sico de **datasette** pero **datasette** es altamente personalizable. Tomemos, por ejemplo, el **datasette** de The Baltimore Sun para que las personas exploren los registros de salarios p√∫blicos que se actualizan anualmente.

![](https://cdn-images-1.medium.com/max/800/1*z4OxKW3TwU4l3AsN2Xm0lQ.png) 

Esto esta corriendo **datasette** detr√°s de c√°maras pero The Baltimore Sun agrego sus propias plantillas y archivos CSS y JS. Para aprender m√°s sobre como personalizar **datasette** visita <https://datasette.readthedocs.io/en/stable/custom_templates.html>

## paso, paso a pasito  

Esta secci√≥n es una descripci√≥n m√°s t√©cnica del proyecto. La preparaci√≥n y despliegue de datos con **datasette** es bastante sencillo y se puede dividir en tres fases:

* Adquirir los datos
* Preparar los datos
* Servir los datos

### adquiriendo los datos

Los datos se obtuvieron del sitio web de Open Justice del Departamento de Justicia de California: <https://openjustice.doj.ca.gov/data>. El sitio web proporciona un enlace para descargar los datos (por lo menos desde 5 de mayo de 2020, cuando lo descargu√© para este proyecto). Para obtener m√°s informaci√≥n sobre los datos en s√≠, puede leer la secci√≥n [Acerca de los datos](https://github.com/chekos/RIPA-2018-datasette#about-the-data) en el repositorio de GitHub del proyecto.

### preparando los datos

El CSV original es de mas de 640 MB as√≠ que el primer paso es dividirlo en 15 archivos para que cada uno pueda subirse a GitHub en este repositorio.

```python
import pandas as pd
import numpy as np
from pathlib import Path

THIS_FILE = Path(__file__)
THIS_DIR = THIS_FILE.parent
EXTERNAL_DATA = THIS_DIR.joinpath("./../../data/external/")
INTERIM_DATA = THIS_DIR.joinpath("./../../data/interim/")

FULL_DB = "RIPA Stop Data 2018.csv"
data = pd.read_csv(EXTERNAL_DATA.joinpath(FULL_DB))

dfs = np.array_split(data, 15)
for idx,df in enumerate(dfs):
    print(f"Saving dataset #{idx}")
    df.to_csv(INTERIM_DATA.joinpath(f"ripa-2018-part-{idx}.csv"), index = False)
```
\- <https://github.com/chekos/RIPA-2018-datasette/blob/fea53ce60ae43f0d7b0dd130109a01496f08e20a/src/data/split_data.py#L1-L16>

> üí°¬ø**Por qu√©**? Nom√°s por si desaparece misteriosamente de la fuente original...

Sin embargo, debido a que el conjunto de datos es demasiado grande para servir como una sola tabla (1,8 millones de filas por 143 columnas), tambi√©n se dividi√≥ en tablas m√°s peque√±as. Esto significa que tomamos variables relacionadas (basadas en sus sufijos) y las extrajimos en sus propias tablas. Por ejemplo, variables relacionadas con el g√©nero como `G_FULL`,` G_MALE`, `G_FEMALE`,` G_TRANSGENDER_MAN`, `G_TRANSGENDER_WOMAN`,`G_GENDER_NONCOMFORMING`, y `G_MULTIGENDER` se extrajeron de la tabla "principal "y se agregaron a una tabla de **g√©nero** en la base de datos.

Estas se pueden volver a unir a la tabla principal utilizando un **UNIQUE_ID** asignado a ellas.

> Este script es demasiado grande para incluir en este blog sin crear una distracci√≥n inecesaria pero puedes ver las 91 l√≠neas de c√≥digo aqu√≠: <https://github.com/chekos/RIPA-2018-datasette/blob/master/src/data/break_down_database.py>

A cada observaci√≥n o fila de este conjunto de datos se le asigna un `DOJ_RECORD_ID` y un` PERSON_NUMBER`. Estos son exclusivos de la parada y las personas detenidas, respectivamente. Esto significa que podr√≠amos combinarlos para crear un 'ID √öNICO' para cada fila que podr√≠amos usar para unir tablas. Sin embargo, esto termina siendo una cadena de 22 caracteres que es innecesariamente grande. Para facilitar las cosas, a cada fila se le asigna una identificaci√≥n num√©rica que comienza en 1,000,000. Comenzar en un mill√≥n es completamente arbitrario, podr√≠amos haber comenzado en cero, pero debido a que hay 1.8 millones de filas, tomamos la decisi√≥n de que cada identificaci√≥n num√©rica sea de siete d√≠gitos. Este `UNIQUE_ID` num√©rico nos permite unir tablas juntas **y** no es una gran adici√≥n a la base de datos en t√©rminos de memoria.

Una vez que se ha creado este `ID_√öNICO`, podemos extraer columnas de la tabla "principal" en sus propias tablas y guardarlas como archivos CSV individuales con la certeza de que cada observaci√≥n puede coincidir en estas nuevas tablas.

Luego usamos `csvs-to-sqlite` para crear una base de datos sqlite donde cada CSV es una tabla. En este paso, tambi√©n incluimos el archivo **Appendix B Table 3.csv** obtenido tambi√©n del sitio web del DOJ y cualquier otra tabla complementaria que podr√≠amos haber creado para acompa√±ar el conjunto de datos.

``` 
csvs-to-sqlite data/processed/*.csv "data/external/Appendix B Table 3.csv" datasette/ripa-2018-db.db
```

### sirviendo los datos

Despu√©s de preparar los datos y crear una base de datos sqlite usamos **datasette** para servirlos como un sitio web interactivo y su API. Esto es tan f√°cil como ejecutar

```shell
datasette ripa-2018.db
```

Pero para este proyecto personalizamos nuestro **datasette** un poco.

Incluimos un t√Ætulo, una descripci√≥n, la url de la fuente de los datos y archivos CSS y JS. Puedes explorar el archivo `metadata.json` en el repositorio [datasette/metadata.json](https://github.com/chekos/RIPA-2018-datasette/blob/master/datasette/metadata.json) para ver que m√°s hicimos.

Tambi√©n inclu√≠mos lo que **datasette** llama _canned queries_, lo que ser√≠a "consultas almacenadas" (?). Estas son consultas de SQL inclu√≠das en tu instancia que aparecen al fondo de tu p√°gina principal y tiene su propio URL para facilitar el acceso. Estas consultas las incluimos porque muestran informaci√≥n √∫til y/o interesante de los datos. Algunas de ellas son consultas que computan informaci√≥n especifica publicada en el reporte anual del 2020.

Tambi√©n modificamos algunas plantillas de **datasette**, especificamente `base.html` y `query.html`. La primera fue modificada para incluir metadatos en la etiqueta `<head>` (descripci√≥n del sitio, por ejemplo, para cuando se comparta un enlace). La segunda fue modificada para incluir un bot√≥n debajo del cuadro de texto donde escribes tus consultas de SQL. Este bot√≥n sirve para facilitar las sugerencias de consultas para el repositorio. Al hacer clic, se abre otra ventana en tu navegador que crea un nuevo _issue_ en github con la consulta de SQL que acabas de ejecutar.

Tambi√©n cambiamos algunas opciones para **datasette**:
1. `default_page_size:50` - Muestra solo 50 resultados por p√°gina
2. `sql_time_limit_ms:30000` - Un l√≠mite de 30 segundos para ejecutar consultas (es el l√≠mite de tiempo de Heroku)
3. `facet_time_limit_ms:10000` - El tiempo l√≠mite que **datasette** deber√≠a usar calculando una posible faceta de tu tabla (el default es de 200ms pero nuestro conjunto de datos es tan grande que lo expandimos a 10 segundos)

El c√≥digo por ejecutar es:
``` shell
datasette ripa-2018-db.db \\
  -m metadata.json \\
  --extra-options="--config default_page_size:50 --config sql_time_limit_ms:30000 --config facet_time_limit_ms:10000"
```

## desplegando en heroku

Aqu√≠ hay una descripci√≥n general de alto nivel del proceso

![<https://github.com/chekos/RIPA-2018-datasette>](https://cdn-images-1.medium.com/max/800/1*DGg6BYW2iexf2SsFrQnpig.png) 

Hasta ahora, utilizamos un c√≥digo de Python para procesar nuestros datos (creando un `UNIQUE_ID` y desglosando el conjunto de datos original con 143 columnas en varios archivos` .csv` m√°s peque√±os) y tambi√©n utilizamos `csvs-to-sqlite` para construir nuestra base de datos `ripa-2018-db.db` de esos archivos` .csv`. Hasta ahora, hemos estado interactuando con nuestro **datasette** ejecutando el comando `datasette ripa-2018-db.db` que ejecuta un servidor local. Para que nuestro **datasette** est√© disponible para el mundo, tenemos que implementarlo en l√≠nea y, por suerte, es muy, muy f√°cil de hacer, ya que **datasette** ya incluye un comando de publicaci√≥n.

Con **datasette** puedes publicar directamente a [Heroku](https://datasette.readthedocs.io/en/stable/publish.html#publishing-to-heroku), [Google Cloud Run](https://datasette.readthedocs.io/en/stable/publish.html#publishing-to-google-cloud-run), o [Fly](https://datasette.readthedocs.io/en/stable/publish.html#publishing-to-fly) (por lo menos desde la versi√≥n 0.42 que es la que utilizamos en este proyecto). Debido a que tengo experiencia previa en la implementaci√≥n de heroku, me pareci√≥ la m√°s f√°cil de las tres, pero todas son excelentes opciones y la documentaci√≥n es f√°cil de seguir.

Implementar en Heroku fue tan f√°cil como ejecutar casi el mismo comando utilizado para servir nuestro **datasette** localmente.

``` shell
datasette publish heroku ripa-2018-db.db \\
  --name ripa-2018-db \\
  -m metadata.json \\
  --extra-options="--config default_page_size:50 --config sql_time_limit_ms:30000 --config facet_time_limit_ms:10000"
```

> üí° **Nota** la bandera `--name` que espcif√≠ca el nombre de nuestra aplicaci√≥n en Heroku. Esto significa que la publicar√° a [ripa-2018-db.herokuapp.com](http://ripa-2018-db.herokuapp.com) y sobreescribir√° cualquier aplicaci√≥n previa ah√≠.

Para ejecutar esto, necesitar√° una [cuenta heroku](https://signup.heroku.com/) e instalar la herramienta de [l√≠nea de comandos de heroku](https://devcenter.heroku.com/articles/heroku-cli). **Datasette** (usando el la herramienta de l√≠nea de comandos) te pedir√° que inicies sesi√≥n y abrir√° una ventana del navegador para que lo haga. Despu√©s de iniciar sesi√≥n, se encargar√° del resto.

¬°Listo! En este punto, hemos publicado con √©xito nuestro **datasette** en heroku y podemos visitar [ripa-2018-db.herokuapp.com](http://ripa-2018-db.herokuapp.com) o [ripa-2018.datasettes.cimarron.io/](http://ripa-2018.datasettes.cimarron.io/)

![<http://ripa-2018.datasettes.cimarron.io/>](https://cdn-images-1.medium.com/max/800/1*fuSRYs18jGBzkvlN60pDLA.gif) 

Este ser√° el final de este proceso para muchas personas. Adquirimos y transformamos con √©xito algunos datos y los desplegamos en la nube para que otros puedan explorar e interactuar con ellos. Tambi√©n hemos incluido una descripci√≥n HTML que describe nuestro **datasette** para los usuarios e incluso algunas _consultas almacenadas_ para que las personas reproduzcan algunos de los hechos publicados en el Informe Anual 2020.

## automatizando todo  

Si bien los datos subyacentes que estas publicando no cambian con frecuencia, es posible que desees automatizar la implementaci√≥n de su instancia de **datasette** por muchas razones. Por ejemplo, cuando comenc√© este proyecto, **datasette** estaba en la versi√≥n 0.40 y en el momento en que escribo esta publicaci√≥n est√° en la versi√≥n 0.42. Para la mayor√≠a de las versiones no necesitar√°s actualizar tu instancia de **datasette**, pero la versi√≥n 0.41 inclu√≠a la capacidad de crear p√°ginas personalizadas ([changelog](https://datasette.readthedocs.io/en/stable/changelog.html#v0-41)).


> For example, adding a template file called `templates/pages/about.html` will result in a new page being served at `/about`  on your instance.

Esto significa que podemos agregar mucho m√°s contexto en nuestra instancia para los usuarios. Tal vez incluya una gu√≠a paso a paso para ayudar a las personas a contribuir al proyecto, otros enlaces √∫tiles o una p√°gina simple para presentarse para que las personas que usan estos datos aprendan un poco m√°s sobre ti.

Tambi√©n es posible que desee incluir m√°s _consultas almacenadas_ o corregir un error ortogr√°fico en su descripci√≥n. Cualquiera sea la raz√≥n, la implementaci√≥n autom√°tica es f√°cil de lograr. En definitiva, todo lo que necesita es ejecutar `datasette heroku publishing`, que es un caso de uso perfecto para las GitHub Actions.

> üö® **_Advertencia / Nota_**: Esta pr√≥xima parte se vuelve mucho m√°s t√©cnica r√°pidamente. GitHub Actions es un tema m√°s avanzado. Si no necesitas / deseas actualizar su instancia de datasette recientemente implementada regularmente, no recomendar√≠a pensar en las GitHub Actions por el momento.

*** 

## GitHub Actions  

> GitHub Actions help you automate your software development workflows in the same place you store code and collaborate on pull requests and issues. You can write individual tasks, called actions, and combine them to create a custom workflow. Workflows are custom automated processes that you can set up in your repository to build, test, package, release, or deploy any code project on GitHub. <br> - [https://help.github.com/en/actions/getting-started-with-github-actions/about-github-actions](https://help.github.com/en/actions/getting-started-with-github-actions/about-github-actions)

No entraremos muy a fondo en las acciones de GitHub en esta publicaci√≥n. Lo que necesitas saber es que con las GitHub Actions puede ejecutar tareas en paralelo o en secuencia, y estos se desencadenan por eventos de GitHub como empujar tu c√≥digo a una _branch_, abrir una _pull request_, comentar en un _issue_ o una combinaci√≥n de muchos.

Estos existen en sus repositorios de GitHub en `.github/workflows/` generalmente en forma de archivos yaml. La estructura b√°sica es la siguiente.

``` yaml
name: Example of simple GitHub Action

on:
  push:
    branches: [master]
 
jobs:
  say-hi:
    runs-on: ubuntu-latest
    steps:
    - name: Echo Hello World
      run: |
        echo "Hello World!"
```

Esta acci√≥n se activar√° cada vez que haya un impulso en su rama maestra y ejecutar√° el trabajo `say-hi` que se ejecuta en una m√°quina virtual (VM) ubuntu disponible. En esa VM ejecutar√° el c√≥digo `echo "Hello World!"`

Puede ver los resultados / registros de sus acciones en la pesta√±a **Actions** de tu repositorio.

![](https://cdn-images-1.medium.com/max/800/1*SSxMvEqcCfOyiWB9hidc8A.png) 

Podr√≠as cambiar facilmente `echo "Hello World!"` con
``` shell
datasette publish heroku ripa-2018-db.db \\
  --name ripa-2018-db \\
  -m metadata.json \\
  --extra-options="--config default_page_size:50 --config sql_time_limit_ms:30000 --config facet_time_limit_ms:10000"
```
o mejor a√∫n puedes guardar tu c√≥digo en un _script_ de bash llamado `heroku_deploy.sh` y ejecutar `sh heroku_deploy.sh` como ejecutar√≠as `echo "Hello World!"`

Afortunadamente, Heroku ya est√° instalado en nuestro corredor de la GitHub Action ([ubuntu-latest](https://github.com/actions/virtual-environments/blob/master/images/linux/Ubuntu1804-README.md)), por lo que todo lo que tenemos que hacer es iniciar sesi√≥n, instalar el complemento `heroku-builds` y ejecutar nuestro _script_ `heroku_deploy.sh`.

> üí° Aprend√≠ sobre la necesidad de instalar `heroku-builds` despu√©s de que las acciones de GitHub fallaran un par de veces. No estoy seguro de que est√© documentado en **datasette** o en la documentaci√≥n de GitHub Actions.

Hasta ahora, nuestra GitHub Action se ve as√≠:
```yaml
name: Example of simple GitHub Action

on:
  push:
    branches: [master]

jobs:
  publish-to-heroku:
    runs-on: ubuntu-latest
    steps:
    - name: Publish to Heroku
      run: |
        heroku container:login && heroku plugins:install heroku-builds && sh heroku_deploy.sh
```

Esto, sin embargo, a√∫n no est√° listo para funcionar. Para automatizar el inicio de sesi√≥n, debe incluir la variable de entorno `HEROKU_API_KEY`. Esto es posible mediante el uso de [GitHub Secrets](https://help.github.com/en/actions/configuring-and-managing-workflows/creating-and-storing-encrypted-secrets).

Para crear tu _API key_ necesitas tener instalada la herramienta de heroku para la l√≠nea de comando localmente. Ejecuta el comando `heroku authorizations:create` y a√±adela a tu repositorio en **Settings > Secrets**. Ll√°malo `HEROKU_API_KEY`.

Para que su GitHub Action tenga acceso a ella, debe agregar la siguiente l√≠nea a su archivo yaml

```yaml
name: Example of simple GitHub Action

on:
  push:
    branches: [master]

jobs:
  publish-to-heroku:
    runs-on: ubuntu-latest
    steps:
    - name: Publish to Heroku
      env: ${{ secrets.HEROKU_API_KEY }}
      run: |
        heroku container:login && heroku plugins:install heroku-builds && sh heroku_deploy.sh
```

¬°Listo! Ahora, cada vez que empujes c√≥digo a tu repositorio en la _branch_ "master", implementar√° una nueva versi√≥n de tu **datasette**. Esto significa que puede actualizar su `metadata.json`, por ejemplo, con una nueva _consulta almacenada_ o agregar una nueva p√°gina a tus` templates/pages`.

Para este proyecto, tengo algunos pasos adicionales (pasos de GitHub Action, es decir) que procesan los datos, construyen la base de datos sqlite3 y la despliegan en heroku cada vez que hay un nuevo "commit" en "master". Tambi√©n incluyo un par de _scripts_ de Python que leen un archivo yaml separado donde tengo todas las _consultas almacenadas_ y las agrego a un `updated_metadata.json`. Esto es para mantener las cosas un poco m√°s limpias, cada _consulta almacenada_ tiene un t√≠tulo, html_description y una larga consulta SQL; Es m√°s f√°cil de mantener como un archivo separado, en mi opini√≥n.

Puedes ver el archivo yaml de mi GitHub Action aqu√≠ <https://github.com/chekos/RIPA-2018-datasette/blob/master/.github/workflows/main.yml>

## bonus
**using github issues to suggest¬†queries**

Si visitas <ripa-2018.datasettes.cimarron.io> y ejecutas una consulta, notar√°s el bot√≥n **Submit in GitHub**.

![](https://cdn-images-1.medium.com/max/800/1*hKuWnful3BZbVRBuiIUOuQ.gif) 

Esto se hace mediante un ajuste de la plantilla `query.html`. **Datasette** usa `jinja2` y en realidad est√° pasando la consulta SQL como un par√°metro de consulta de URL, lo que significa que puede acceder a √©l usando `request.query_string.split('sql =')[- 1]`

Ya que tienes acceso a la consulta es facil crear un enlace directo a un _issue_ nuevo en tu repo
```ruby
{% set link_to_new_issue = "<https://GitHub.com/><YOUR_USERNAME>/<YOUR_REPO>/issues/new?title=Query+suggestion&labels=suggestion&body=" + <QUERY_FOR_ISSUE> %}
```

As√≠ se ve en `query.html`
```html
...
    {% set query_for_issue = "%23+title%0A%0A%23+query%0A%60%60%60sql%0A" + request.query_string.split('sql=')[-1] + "%0A%60%60%60" %}
    {% set link_to_new_issue = "https://GitHub.com/chekos/ripa-2018-datasette/issues/new?title=Query+suggestion&labels=suggestion&body=" + query_for_issue %}

    <p>
        <button id="sql-format" type="button" hidden>Format SQL</button>
        <input type="submit" value="Run SQL">
        <button type="button" class="btn btn-secondary btn-sm"><a href="{{ link_to_new_issue }}" target="_blank">Submit on <i class="fab fa-github"></i></a></button>
    </p>
...
```

## cuanto tiempo tom√≥

Todo el proceso tom√≥ alrededor de 20 horas en total, distribuidas en 3 a 4 semanas. La mayor parte estaba planeando y orquestando todo el proceso automatizado usando GitHub Actions. Espero que este tutorial te ayude a ahorrar esas horas extra. Al igual que cualquier otro proyecto de datos, es mejor pasar un tiempo por adelantado pensando y planeando detalladamente cada paso del proceso.

## llamado a la acci√≥n

El mundo ha cambiado. La idea de este proyecto se me ocurri√≥ a mediados de abril y no cre√© el repositorio de GitHub hasta finales de mes. No comenc√© a escribir este tutorial hasta la segunda mitad de mayo. Todo ha cambiado desde entonces.

Todos los d√≠as en las redes sociales vemos videos y leemos historias sobre autoridades que abusan de su poder y ejercen violencia contra personas inocentes. No son todos, pero no deber√≠a ser ninguno de ellos.

Creo en los datos como una herramienta para el cambio, para la rendici√≥n de cuentas, para la transparencia. El mundo ha cambiado para siempre por la tecnolog√≠a y los datos. Como tecn√≥logos, nerds de datos, expertos en pol√≠ticas o cualquier etiqueta autoascrita que elijan, tu que me lees, tenemos la responsabilidad de que nuestros pares usen nuestras habilidades, nuestras poderosas herramientas, nuestro conocimiento para hacer del mundo un lugar mejor **porque podemos**.

Te pido que pienses en el papel que desempe√±as. ¬øHar√°s del mundo un lugar mejor con las herramientas que est√°s construyendo, por favor?

Te dejo con el siguiente pensamiento de [Data4BlackLives](http://d4bl.org/):

> **Data for Black Lives** es un movimiento de activistas, organizadores y matem√°ticos comprometidos con la misi√≥n de utilizar la ciencia de datos para crear un cambio concreto y medible en la vida de las personas negras. **Desde el advenimiento de la inform√°tica, la big data y los algoritmos han penetrado pr√°cticamente todos los aspectos de nuestra vida social y econ√≥mica. Estos nuevos sistemas de datos tienen un enorme potencial para empoderar a las comunidades de color. Herramientas como el modelado estad√≠stico, la visualizaci√≥n de datos y el abastecimiento p√∫blico, en las manos correctas, son instrumentos poderosos para combatir el sesgo, construir movimientos progresivos y promover el compromiso c√≠vico. Pero la historia cuenta una historia diferente, una en la que los datos se utilizan con demasiada frecuencia como un instrumento de opresi√≥n, que refuerza la desigualdad y perpet√∫a la injusticia. Redlining fue una empresa basada en datos que result√≥ en la exclusi√≥n sistem√°tica de las comunidades negras de los servicios financieros clave. Las tendencias m√°s recientes como la vigilancia predictiva, las sentencias basadas en el riesgo y los pr√©stamos abusivos son variaciones preocupantes sobre el mismo tema. Hoy, la discriminaci√≥n es una empresa de alta tecnolog√≠a.** <br> - [_d4bl.org_](http://d4bl.org/) 

***
Este art√≠culo fue publicado originalmente en ingl√©s en la publicaci√≥n Towards Data Science, [_Making open data more accessible with datasette_](https://medium.com/@chekos/making-open-data-more-accessible-with-datasette-480a1de5e919) 
